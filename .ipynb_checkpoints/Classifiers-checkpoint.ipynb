{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "datasets = r'C:\\Users\\ISHAASAMYUKTHA\\iCloudDrive\\Desktop\\FYP-EPILEPSY\\CD DATA\\cdicandnormal.xlsx'\n",
    "\n",
    "dataset = pd.read_excel(datasets, header=None)\n",
    "\n",
    "feature_names = dataset.iloc[0,0:76]\n",
    "# attributes\n",
    "X = dataset.iloc[1:80, 0:76]\n",
    "\n",
    "# label(result)\n",
    "y = dataset.iloc[1:80, 76]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 4)\n",
    "# Scaling to bring values to the same range\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4375, 0.5   , 0.375 , 0.4375, 0.5   , 0.5   , 0.5625, 0.5   ,\n",
       "       0.375 ])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################### KNN MODEL ############################################################\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "Ks = 10\n",
    "mean_acc = np.zeros((Ks-1))\n",
    "std_acc = np.zeros((Ks-1))\n",
    "for n in range(1,Ks):\n",
    "    neigh = KNeighborsClassifier(n_neighbors = n).fit(X_train,y_train)\n",
    "    yhat = neigh.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,Ks),mean_acc,'m')\n",
    "plt.fill_between(range(1,Ks),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = neigh.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################## DECISION TRESS ##################################################################################\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "max_depth = 10\n",
    "mean_acc = np.zeros((max_depth-1))\n",
    "std_acc = np.zeros((max_depth-1))\n",
    "for n in range(1,max_depth):\n",
    "    dec = DecisionTreeClassifier(criterion='entropy', max_depth=n).fit(X_train,y_train)\n",
    "    yhat = dec.predict(X_test)\n",
    "    mean_acc[n-1] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[n-1] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "    \n",
    "    mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,max_depth),mean_acc,'g')\n",
    "plt.fill_between(range(1,max_depth),mean_acc - 1 * std_acc,mean_acc + 1 * std_acc, alpha=0.10)\n",
    "plt.legend(('Accuracy ', '+/- 3xstd'))\n",
    "plt.ylabel('Accuracy ')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth 1 or 2 seems overly simplistic, let's choose depth = 6 as the best model\n",
    "dec_tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=6).fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ SVM ##########################################3\n",
    "from sklearn import svm\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "mean_acc = {}\n",
    "std_acc = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    neigh = svm.SVC(kernel=kernel).fit(X_train,y_train)\n",
    "    yhat = neigh.predict(X_test)\n",
    "    mean_acc[kernel] = metrics.accuracy_score(y_test, yhat)\n",
    "    std_acc[kernel] = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "\n",
    "mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the linear model seem to give the best result\n",
    "svm_model = svm.SVC(kernel=kernel).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################## LOGISTIC REGRESSION ##################################################\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "regularisations = [1, 0.3, 0.1, 0.03, 0.01, 0.003, 0.001]\n",
    "solver_mean_acc = {}\n",
    "solver_std_acc = {}\n",
    "solver_best_reg = {}\n",
    "for solver in solvers:\n",
    "    best_mean = 0\n",
    "    best_std = 0\n",
    "    best_reg = 0\n",
    "    for reg in regularisations:\n",
    "        lr = LogisticRegression(C=reg, solver=solver).fit(X_train, y_train)\n",
    "        yhat = lr.predict(X_test)\n",
    "        mean = metrics.accuracy_score(y_test, yhat)\n",
    "        std = np.std(yhat==y_test)/np.sqrt(yhat.shape[0])\n",
    "        if mean > best_mean:\n",
    "            best_mean = mean\n",
    "            best_std = std\n",
    "            best_reg = reg\n",
    "    solver_mean_acc[solver] = best_mean\n",
    "    solver_std_acc[solver] = best_std\n",
    "    solver_best_reg[solver] = best_reg\n",
    "\n",
    "solver_mean_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_best_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(C=0.3, solver='lbfgs').fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################### RANDOM FOREST ##################################################################\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from pprint import pprint\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 10, stop = 100, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def evaluate(model, X_test, y_test):\n",
    "    yhat = model.predict(X_test)\n",
    "   # accuracy: (tp + tn) / (p + n)\n",
    "    accuracy = accuracy_score(y_test, yhat.round())\n",
    "    print('Accuracy: %f' % accuracy)\n",
    "    # precision tp / (tp + fp)\n",
    "    precision = precision_score(y_test, yhat.round())\n",
    "    print('Precision: %f' % precision)\n",
    "    # recall: tp / (tp + fn)\n",
    "    recall = recall_score(y_test, yhat.round())\n",
    "    print('Recall: %f' % recall)\n",
    "    # f1: 2 tp / (2 tp + fp + fn)\n",
    "    f1 = f1_score(y_test, yhat.round(),'weighted')\n",
    "    print('F1 score: %f' % f1)\n",
    "    # Jaccard Index\n",
    "    jaccard=jaccard_score(y_test, yhat.round(),'weighted')\n",
    "    print('Jaccard: %f' % jaccard)\n",
    "    # kappa\n",
    "    kappa = cohen_kappa_score(y_test, yhat.round())\n",
    "    print('Cohens kappa: %f' % kappa)\n",
    "    # ROC AUC\n",
    "    auc = roc_auc_score(y_test, yhat.round())\n",
    "    print('ROC AUC: %f' % auc)\n",
    "    # confusion matrix\n",
    "    matrix = confusion_matrix(y_test, yhat.round())\n",
    "    print(matrix)\n",
    "    \n",
    "    return accuracy,precision,recall,f1,jaccard,kappa,auc,matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################# NAIVE BAYES ##############################################\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "yhat = gnb.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(index=['KNN', 'Decision Tree', 'SVM', 'Logistic Regression','Random Forest', 'Naive Bayes'], columns=['Jaccard', 'F1-score', 'LogLoss'])\n",
    "\n",
    "yhat = neigh.predict(X_test)\n",
    "mean = metrics.accuracy_score(y_test, yhat)\n",
    "(prec, recall, f1, support) = precision_recall_fscore_support(y_test, yhat, average='weighted')\n",
    "df.loc['KNN'] = [mean, f1, np.NaN]\n",
    "\n",
    "yhat = dec_tree_model.predict(X_test)\n",
    "mean = metrics.accuracy_score(y_test, yhat)\n",
    "(prec, recall, f1, support) = precision_recall_fscore_support(y_test, yhat, average='weighted')\n",
    "df.loc['Decision Tree'] = [mean, f1, np.NaN]\n",
    "\n",
    "yhat = svm_model.predict(X_test)\n",
    "mean = metrics.accuracy_score(y_test, yhat)\n",
    "(prec, recall, f1, support) = precision_recall_fscore_support(y_test, yhat, average='weighted')\n",
    "df.loc['SVM'] = [mean, f1, np.NaN]\n",
    "\n",
    "yhat = lr_model.predict(X_test)\n",
    "mean = metrics.accuracy_score(y_test, yhat)\n",
    "(prec, recall, f1, support) = precision_recall_fscore_support(y_test, yhat, average='weighted')\n",
    "yhat_prob = lr_model.predict_proba(X_test)\n",
    "ll = log_loss(y_test, yhat_prob)\n",
    "df.loc['Logistic Regression'] = [mean, f1, ll]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
